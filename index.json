[{"authors":null,"categories":null,"content":"I am a Research Scientist at NVIDIA in the Toronto AI Lab, generally interested in 3D computer vision and neural reconstruction.\nPrior to that I obtained my PhD at ETH Zurich in the EcoVision Lab, Photogrammetry and Remote Sensing Group under the supervision of Prof. Jan Wegner and Prof. Konrad Schindler. During my PhD I worked on super-resolution, depth upsampling/completion, fine-grained classification and biodiversity monitoring.\n Download my CV. -- ","date":1717891200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717891200,"objectID":"9247cda5d4b8717d974b6bb9d64010b1","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Research Scientist at NVIDIA in the Toronto AI Lab, generally interested in 3D computer vision and neural reconstruction.\nPrior to that I obtained my PhD at ETH Zurich in the EcoVision Lab, Photogrammetry and Remote Sensing Group under the supervision of Prof.","tags":null,"title":"Riccardo de Lutio","type":"authors"},{"authors":null,"categories":null,"content":"üëã Welcome to the Academic Template The Wowchemy Academic Resum√© Template for Hugo empowers you to create your job-winning online resum√© and showcase your academic publications.\nCheck out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase.\nWowchemy makes it easy to create a beautiful website for free. Edit your site in Markdown, Jupyter, or RStudio (via Blogdown), generate it with Hugo, and deploy with GitHub or Netlify. Customize anything on your site with widgets, themes, and language packs.\n üëâ Get Started üìö View the documentation üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to unlock rewards with sponsorship You‚Äôre looking at a Wowchemy widget  This homepage section is an example of adding elements to the Blank widget.\nBackgrounds can be applied to any section. Here, the background option is set give a color gradient.\nTo remove this section, delete content/home/demo.md.\n  Get inspired Check out the Markdown files which power the Academic Demo, or view the showcase.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1d1825344e8f4b25c2137e0a9c8b655f","permalink":"https://riccardodelutio.github.io/home-unused/demo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/demo/","section":"home-unused","summary":"üëã Welcome to the Academic Template The Wowchemy Academic Resum√© Template for Hugo empowers you to create your job-winning online resum√© and showcase your academic publications.\nCheck out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase.","tags":null,"title":"Academic Template","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4a7e3501655fed0a4b0ce814e15ff2c9","permalink":"https://riccardodelutio.github.io/home-unused/skills/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/skills/","section":"home-unused","summary":"","tags":null,"title":"Skills","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0e643989bdefe366f2b5fddf949a36b6","permalink":"https://riccardodelutio.github.io/home-unused/posts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/posts/","section":"home-unused","summary":"","tags":null,"title":"Additional Activities","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"89201c04ad04664a30c3fb9ba7b170aa","permalink":"https://riccardodelutio.github.io/home-unused/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/projects/","section":"home-unused","summary":"","tags":null,"title":"Projects","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d927b251d3da15a737d1f66fb88d4504","permalink":"https://riccardodelutio.github.io/home-unused/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/talks/","section":"home-unused","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"28f54f6e819207239a6024bbaa9d78de","permalink":"https://riccardodelutio.github.io/home-unused/featured/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/featured/","section":"home-unused","summary":"","tags":null,"title":"Featured Publications","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"657179738bed56748434d6ae76e8a647","permalink":"https://riccardodelutio.github.io/home-unused/tags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/tags/","section":"home-unused","summary":"","tags":null,"title":"Popular Topics","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6011477b6d615d7a4005aee5356c1e97","permalink":"https://riccardodelutio.github.io/home-unused/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-unused/contact/","section":"home-unused","summary":"","tags":null,"title":"Contact","type":"home-unused"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://riccardodelutio.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Ziyu Chen","Jiawei Yang","Jiahui Huang","riccardo","Janick Martinez Esturo","Boris Ivanovic","Or Litany","Zan Gojcic","Sanja Fidler","Marco Pavone","Li Song","Yue Wang"],"categories":null,"content":"","date":1738540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738540800,"objectID":"273afd27b96faf41968421df942712c6","permalink":"https://riccardodelutio.github.io/publication/omnire/","publishdate":"2025-02-03T00:00:00Z","relpermalink":"/publication/omnire/","section":"publication","summary":"We introduce OmniRe, a holistic approach for efficiently reconstructing high-fidelity dynamic urban scenes from on-device logs. Recent methods for modeling driving sequences using neural radiance fields or Gaussian Splatting have demonstrated the potential of reconstructing challenging dynamic scenes, but often overlook pedestrians and other non-vehicle dynamic actors, hindering a complete pipeline for dynamic urban scene reconstruction. To that end, we propose a comprehensive 3DGS framework for driving scenes, named OmniRe, that allows for accurate, full-length reconstruction of diverse dynamic objects in a driving log. OmniRe builds dynamic neural scene graphs based on Gaussian representations and constructs multiple local canonical spaces that model various dynamic actors, including vehicles, pedestrians, and cyclists, among many others. This capability is unmatched by existing methods. OmniRe allows us to holistically reconstruct different objects present in the scene, subsequently enabling the simulation of reconstructed scenarios with all actors participating in real-time (~60Hz). Extensive evaluations on the Waymo dataset show that our approach outperforms prior state-of-the-art methods quantitatively and qualitatively by a large margin. We believe our work fills a critical gap in driving reconstruction.","tags":[],"title":"OmniRe: Omni Urban Scene Reconstruction","type":"publication"},{"authors":["John Y. Park","Riccardo de Lutio","Brendan Rapazzo","Barbara A. Ambrose","Fabian Michelangeli","Kimberly A. Watson","Serge J. Belongie","Damon P. Little"],"categories":null,"content":"","date":1717891200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717891200,"objectID":"abaecb8b4f2cdd17985ecbd397996311","permalink":"https://riccardodelutio.github.io/publication/naflora/","publishdate":"2024-06-09T00:00:00Z","relpermalink":"/publication/naflora/","section":"publication","summary":"The plant kingdom exhibits remarkable diversity that must be maintained for global ecosystem sustainability. However, plant life is currently disproportionately disappearing at a rapid rate, putting many essential functions-such as ecosystem production, resistance, and resilience-at risk. Plant specimen identification-the first step of plant biodiversity research-is heavily bottlenecked by a shortage of qualified experts. The botanical community has imaged large volumes of annotated physical herbarium specimens, which present a huge potential for building artificial intelligence systems that can assist researchers. In this paper, we present a novel large-scale, fine-grained dataset, NAFlora-1M, which consists of 1,050,182 hebarium images covering 15,501 North American vascular plant species (90 of the known species). Addressing gaps from previous research efforts, NAFlora-1M is the first‚Äìever dataset to closely replicate the real-world task of herbarium specimen identification, as the dataset is intended to cover as many of the taxa in North America as possible. We highlight some key characteristics of NAFlora-1M from a machine learning dataset perspective: high-quality labels rigorously peer-reviewed by experts; hierarchical class structure; long‚Äìtailed and imbalanced class distribution; high image resolution; and extensive image quality control for consistent scale and color. In addition, we present several baseline models, along with benchmarking results from a Kaggle competition: A total of 134 teams benchmarked the dataset in a total of 1,663 submissions; the leading team achieved an 87.66 macro-F score with a 1‚Äìbillion‚Äìparameter ensemble model‚Äîleaving substantial room for future improvement in both performance and efficiency. We believe that NAFlora1M is an excellent starting point to encourage the development of botanical AI applications, thereby facilitating enhanced monitoring of plant diversity and conservation efforts. The dataset and training scripts are available at https://github.com/dpl10/NAFlora-1M.","tags":[],"title":"NAFlora-1M: Continental-Scale High-Resolution Fine-Grained Plant Classification Dataset","type":"publication"},{"authors":["Philipp Brun","Dirk N. Karger","Damaris Zurell","Patrice Descombes","Lucienne de Witte","Riccardo de Lutio","Jan D. Wegner","Niklaus E. Zimmermann"],"categories":null,"content":"","date":1716508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716508800,"objectID":"cd7b46f0cd986abd79d89bb2d5c4a497","permalink":"https://riccardodelutio.github.io/publication/multispecies/","publishdate":"2024-05-24T00:00:00Z","relpermalink":"/publication/multispecies/","section":"publication","summary":"In the age of big data, scientific progress is fundamentally limited by our capacity to extract critical information. Here, we map fine-grained spatiotemporal distributions for thousands of species, using deep neural networks (DNNs) and ubiquitous citizen science data. Based on 6.7‚ÄâM observations, we jointly model the distributions of 2477 plant species and species aggregates across Switzerland with an ensemble of DNNs built with different cost functions. We find that, compared to commonly-used approaches, multispecies DNNs predict species distributions and especially community composition more accurately. Moreover, their design allows investigation of understudied aspects of ecology. Including seasonal variations of observation probability explicitly allows approximating flowering phenology; reweighting predictions to mirror cover-abundance allows mapping potentially canopy-dominant tree species nationwide; and projecting DNNs into the future allows assessing how distributions, phenology, and dominance may change. Given their skill and their versatility, multispecies DNNs can refine our understanding of the distribution of plants and well-sampled taxa in general.","tags":[],"title":"Multispecies Deep Learning Using Citizen Science Data Produces More Informative Plant Community Models","type":"publication"},{"authors":["Ashkan Mirzaei","Riccardo de Lutio","Seung Wook Kim","David Acuna","Jonathan Kelly","Sanja Fidler","Igor Gilitschenski","Zan Gojcic"],"categories":null,"content":"","date":1713225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713225600,"objectID":"7d6ce2052cc0319521352f632ef70f78","permalink":"https://riccardodelutio.github.io/publication/reffusion/","publishdate":"2024-04-16T00:00:00Z","relpermalink":"/publication/reffusion/","section":"publication","summary":"Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge. In this work, we propose an approach for 3D scene inpainting -- the task of coherently replacing parts of the reconstructed scene with desired content. Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content. A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control. Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal. Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view. The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details. Our framework achieves state-of-the-art results for object removal while maintaining high controllability. We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction.","tags":[],"title":"RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting","type":"publication"},{"authors":["Riccardo de Lutio","Alexander Becker","Stefano D'Aronco","Stefania Russo","Jan D. Wegner","Konrad Schindler"],"categories":null,"content":"","date":1643846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643846400,"objectID":"f4038d2098b9c24d74d34b678c32e34f","permalink":"https://riccardodelutio.github.io/publication/learning-graph/","publishdate":"2022-02-03T00:00:00Z","relpermalink":"/publication/learning-graph/","section":"publication","summary":"We introduce a novel formulation for guided super-resolution. Its core is a differentiable optimisation layer that operates on a learned affinity graph. The learned graph potentials make it possible to leverage rich contextual information from the guide image, while the explicit graph optimisation within the architecture guarantees rigorous fidelity of the high-resolution target to the low-resolution source. With the decision to employ the source as a constraint rather than only as an input to the prediction, our method differs from state-of-the-art deep architectures for guided super-resolution, which produce targets that, when downsampled, will only approximately reproduce the source. This is not only theoretically appealing, but also produces crisper, more natural-looking images. A key property of our method is that, although the graph connectivity is restricted to the pixel lattice, the associated edge potentials are learned with a deep feature extractor and can encode rich context information over large receptive fields. By taking advantage of the sparse graph connectivity, it becomes possible to propagate gradients through the optimisation layer and learn the edge potentials from data. We extensively evaluate our method on several datasets, and consistently outperform recent baselines in terms of quantitative reconstruction errors, while also delivering visually sharper outputs. Moreover, we demonstrate that our method generalises particularly well to new datasets not seen during training.","tags":[],"title":"Learning Graph Regularisation for Guided Super-Resolution","type":"publication"},{"authors":["Riccardo de Lutio","John Y. Park","Kimberly A. Watson","Stefano D'Aronco","Jan D. Wegner","Jan J. Wieringa","Melissa Tulig","Richard L. Pyle","Timothy J. Gallaher","Gillian Brown","Gordon Guymer","Andrew Franks","Dhahara Ranatunga","Yumiko Baba","Serge J. Belongie","Fabian Michelangeli","Barbara A. Ambrose","Damon P. Little"],"categories":null,"content":"","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643673600,"objectID":"71e2ae4d95e8cddf8575177b7f7b88ba","permalink":"https://riccardodelutio.github.io/publication/herbarium/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/publication/herbarium/","section":"publication","summary":"Herbarium sheets present a unique view of the world's botanical history, evolution, and biodiversity. This makes them an all‚Äìimportant data source for botanical research. With the increased digitization of herbaria worldwide and advances in the domain of fine‚Äìgrained visual classification which can facilitate automatic identification of herbarium specimen images, there are many opportunities for supporting and expanding research in this field. However, existing datasets are either too small, or not diverse enough, in terms of represented taxa, geographic distribution, and imaging protocols. Furthermore, aggregating datasets is difficult as taxa are recognized under a multitude of names and must be aligned to a common reference. We introduce the Herbarium 2021 Half‚ÄìEarth dataset: the largest and most diverse dataset of herbarium specimen images, to date, for automatic taxon recognition. We also present the results of the Herbarium 2021 Half‚ÄìEarth challenge, a competition that was part of the Eighth Workshop on Fine-Grained Visual Categorization (FGVC8) and hosted by Kaggle to encourage the development of models to automatically identify taxa from herbarium sheet images.","tags":[],"title":"The Herbarium 2021 Half‚ÄìEarth Challenge Dataset and Machine Learning Competition","type":"publication"},{"authors":["Riccardo de Lutio","Yihang She","Stefano D'Aronco","Stefania Russo","Philipp Brun","Jan D. Wegner","Konrad Schindler"],"categories":null,"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"4eca40fa886d92a1d9eb1ff04b114a13","permalink":"https://riccardodelutio.github.io/publication/digital-taxonomist/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/publication/digital-taxonomist/","section":"publication","summary":"","tags":[],"title":"Digital Taxonomist: Identifying Plant Species in Community Scientists' Photographs","type":"publication"},{"authors":["Riccardo de Lutio"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"401dc030c45d30eab6368fd500320c2e","permalink":"https://riccardodelutio.github.io/post/medium/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/post/medium/","section":"post","summary":"","tags":null,"title":"Co-Editor of the Medium blog of the EcoVision Lab","type":"post"},{"authors":["Riccardo de Lutio","Stefano D'Aronco","Jan D. Wegner","Konrad Schindler"],"categories":null,"content":"","date":1569888e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888e3,"objectID":"b06dc43021dbc6358e974e9b15131a80","permalink":"https://riccardodelutio.github.io/publication/pixtransform/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/pixtransform/","section":"publication","summary":"Guided super-resolution is a unifying framework for several computer vision tasks where the inputs are a lowresolution source image of some target quantity (e.g., perspective depth acquired with a time-of-flight camera) and a high-resolution guide image from a different domain (e.g., a grey-scale image from a conventional camera); and the target output is a high-resolution version of the source (in our example, a high-res depth map). The standard way of looking at this problem is to formulate it as a super-resolution task, i.e., the source image is upsampled to the target resolution, while transferring the missing high-frequency details from the guide. Here, we propose to turn that interpretation on its head and instead see it as a pixel-to-pixel mapping of the guide image to the domain of the source image. The pixel-wise mapping is parametrised as a multi-layer perceptron, whose weights are learned by minimising the discrepancies between the source image and the downsampled target image. Importantly, our formulation makes it possible to regularise only the mapping function, while avoiding regularisation of the outputs; thus producing crisp, naturallooking images. The proposed method is unsupervised, using only the specific source and guide images to fit the mapping. We evaluate our method on two different tasks, superresolution of depth maps and of tree height maps. In both cases we clearly outperform recent baselines in quantitative comparisons, while delivering visually much sharper outputs.","tags":[],"title":"Guided Super-Resolution as Pixel-to-Pixel Transformation","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}   Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://riccardodelutio.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://riccardodelutio.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["admin","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ad492be32314d44a152c92d37ce06386","permalink":"https://riccardodelutio.github.io/home-unused/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/home-unused/example/","section":"home-unused","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"home-unused"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://riccardodelutio.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]